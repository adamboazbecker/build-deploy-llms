import os
import openai
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
from getpass import getpass
from config import config

import langchain
from dotenv import load_dotenv

load_dotenv()  # take environment variables from .env.

QUESTION = "How should I think about artifacts?"


def read_openai_api_key():
  """
  Reads the OpenAI API key from environment variables or user input.

  This function first attempts to retrieve the OpenAI API key from the environment variable
  "OPENAI_API_KEY." If the environment variable is not set, it prompts the user to input
  their API key interactively. The API key is then validated to ensure it starts with "sk-"
  to confirm its validity, and it is set as the authentication key for OpenAI API calls.

  Args:
      None

  Raises:
      AssertionError: If the provided API key does not start with "sk-", indicating an
                       invalid OpenAI API key.

  Returns:
      None
  """
  api_key = os.environ.get("OPENAI_API_KEY", None)
  if api_key is None:
    api_key = getpass("Paste your OpenAI key from: https://platform.openai.com/account/api-keys\n")

  assert api_key.startswith("sk-"), "This doesn't look like a valid OpenAI API key"
  openai.api_key = api_key


def download_from_wandb_artifact():
  """
  Downloads an artifact from Weights and Biases (WandB) and returns its local directory.

  This function initializes a WandB run, retrieves the specified artifact using the provided
  configuration, and downloads it to the local directory. The artifact typically contains
  data or files required for a specific task, such as a search index.

  Args:
      None

  Returns:
      str: The local directory path where the downloaded artifact is stored.

  Note:
      Make sure that the WandB library is properly configured with the necessary API keys
      and project information before using this function.
  """
  run = wandb.init()
  artifact = run.use_artifact(config["vector_store_artifact"], type="search_index")
  artifact_dir = artifact.download()
  return artifact_dir


def load_vector_store(vector_store_path) -> langchain.vectorstores.Chroma:
  """Load a vector store from a Weights & Biases artifact
  Args:
      vector_store_path (str): The path to the vector store
  Returns:
      Chroma: A chroma vector store object
  """
  embedding_fn = langchain.embeddings.OpenAIEmbeddings(openai_api_key=openai.api_key)
  # load vector store
  vector_store = langchain.vectorstores.Chroma(
    embedding_function=embedding_fn, persist_directory=vector_store_path
  )
  return vector_store


def get_relevant_documents(query, vector_store):
  """
  Retrieves relevant documents from a vector store based on a given query.

  Args:
      query (str): The query string to search for relevant documents.
      vector_store (VectorStore): An instance of the VectorStore class containing document vectors.

  Returns:
      list: A list of relevant documents retrieved from the vector store.

  Prints:
      Prints the source metadata of each relevant document to the console.
  """
  docs = vector_store.get_relevant_documents(query)
  return docs


def get_stuffed_prompt(docs, query):
  """
  Generates a prompt for answering a question based on a set of context documents.

  This function takes a list of documents and a query, and generates a structured prompt
  using a predefined template. The context from the provided documents is formatted and
  included in the prompt to help answer the question at the end.

  Args:
      docs (list): A list of document objects or text snippets serving as context.
      query (str): The question or query to be answered based on the provided context.

  Returns:
      str: A formatted prompt for answering the question based on the context.

  Note:
      The generated prompt template is intended for use with a PromptTemplate object.
  """
  prompt_template = """Use the following pieces of context to answer the question at the end.
  If you don't know the answer, just say that you don't know, don't try to make up an answer.

  {context}

  Question: {question}
  Helpful Answer:"""
  PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
  )

  context = "\n\n".join([doc.page_content for doc in docs])
  prompt = PROMPT.format(context=context, question=query)
  return prompt


def call_openai_chat(prompt):
  """
  Calls the OpenAI chat model to generate a response based on the given prompt.

  This function initializes an instance of the OpenAI chat model and sends the provided
  prompt to the model for generating a response.

  Args:
      prompt (str): The input prompt or query to be used for generating a response.

  Returns:
      str: The response generated by the OpenAI chat model.

  Note:
      Make sure that the OpenAI model is properly configured and authenticated before
      using this function, and that you have the necessary API keys or credentials.
  """
  llm = OpenAI()
  response = llm.predict(prompt)
  return response


def set_up_logging():
  """
  Configures logging and tracing for WandB integration.

  This function sets environment variables to enable logging and tracing for WandB (Weights and Biases)
  integration. It enables the Langchain tracing for WandB and configures the WandB project name.

  Args:
      None

  Note:
      - Setting "LANGCHAIN_WANDB_TRACING" to "true" enables Langchain tracing with WandB.
      - Setting "WANDB_PROJECT" specifies the WandB project name for logging.

  See Also:
      WandB documentation for advanced environment variable configuration:
      https://docs.wandb.ai/guides/track/advanced/environment-variables
  """
  # What if we want to log and see everything in WandB afterwards?
  # we need a single line of code to start tracing langchain with W&B
  os.environ["LANGCHAIN_WANDB_TRACING"] = "true"

  # wandb documentation to configure wandb using env variables
  # https://docs.wandb.ai/guides/track/advanced/environment-variables
  # here we are configuring the wandb project name
  os.environ["WANDB_PROJECT"] = "llmapps"


def retrieve_with_chain(question, vector_store):
  """
  Performs a retrieval-based question-answering using Langchain.

  This function utilizes the Langchain library to set up a retrieval-based question-answering
  pipeline. It converts the provided vector_store into a retriever and uses the Langchain's
  RetrievalQA class to answer the given question.

  Args:
      question (str): The question to be answered based on the provided vector_store.
      vector_store (VectorStore): An instance of the VectorStore containing document vectors.

  Returns:
      None

  Note:
      - This function demonstrates how to set up a retrieval-based QA pipeline using Langchain.
      - The specific implementation of converting vector_store into a retriever is missing
        (replace 'retriever=...' with the appropriate code).
  """
  # Clue! We have a vector store, yet the chain requires a "retriever".
  # How do we convert a vector store into a "retriever"?
  qa = langchain.chains.RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=...
  )
  result = qa.run(question)
  print(result)


def main():
  # Step 1:
  read_openai_api_key()

  # Step 2:
  vector_store_path = download_from_wandb_artifact()

  # Step 3:
  vector_store = load_vector_store(vector_store_path=vector_store_path)

  # Step 4:
  # For the next step, pick either the manual approach (#1), or the chain approach (#2)

  chosen_approach = "approach_1"
  if chosen_approach == "approach_1":
    # Approach 1: "Stuff" the prompt yourself manually
    # documents = get_relevant_documents(QUESTION, vector_store)
    # stuffed_prompt = get_stuffed_prompt(documents, question)
    # call_openai_chat(stuffed_prompt)
    pass
  elif chosen_approach == "approach_2":
    # Approach 2: Use a chain instead with logging
    # set_up_logging()
    # retrieve_with_chain(question, vector_store)
    pass


if __name__ == "__main__":
  main()
